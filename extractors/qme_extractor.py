"""
QME/AME/IME Enhanced Extractor - Full Context
Optimized for accuracy using Gemini-style full-document processing
"""
import logging
import re
import time
from typing import Dict, Optional, List
from langchain_core.output_parsers import JsonOutputParser
from langchain_core.prompts import ChatPromptTemplate, SystemMessagePromptTemplate, HumanMessagePromptTemplate
from langchain_openai import AzureChatOpenAI

from models.data_models import ExtractionResult
from utils.extraction_verifier import ExtractionVerifier
from utils.summary_helpers import ensure_date_and_author

logger = logging.getLogger("document_ai")


class QMEExtractorChained:
    """
    Enhanced QME extractor with FULL CONTEXT processing.
    
    Key Features:
    - Full document context (no chunking) = No information loss
    - Uses raw text processing = Simplified pipeline
    - Chain-of-thought reasoning = Explains decisions
    - Optimized for accuracy matching Gemini's approach
    """

    def __init__(self, llm: AzureChatOpenAI, mode):
        self.llm = llm
        self.parser = JsonOutputParser()
        self.verifier = ExtractionVerifier(llm)
        self.mode = mode
        # Pre-compile regex for efficiency
        self.medical_credential_pattern = re.compile(
            r'\b(dr\.?|doctor|m\.?d\.?|d\.?o\.?|mbbs|m\.?b\.?b\.?s\.?)\b',
            re.IGNORECASE
        )
        
        logger.info("âœ… QMEExtractorChained initialized (Full Context + Raw Text)")

    def extract(
        self,
        text: str,
        raw_text: str,
        doc_type: str,
        fallback_date: str
    ) -> Dict:
        """
        Extract QME data with FULL CONTEXT using raw text.
        
        Args:
            text: Complete document text (layout-preserved)
            doc_type: Document type (QME/AME/IME)
            fallback_date: Fallback date if not found
            raw_text: summarized original context ()
            
        Returns:
            Dict with long_summary and short_summary
        """
        logger.info("=" * 80)
        logger.info("ðŸ¥ STARTING QME EXTRACTION (FULL CONTEXT + RAW TEXT)")
        logger.info("=" * 80)
        
        
        # Check document size
        text_length = len(raw_text)
        token_estimate = text_length // 4
        logger.info(f"ðŸ“„ Document size: {text_length:,} chars (~{token_estimate:,} tokens)")
        
        if token_estimate > 120000:
            logger.warning(f"âš ï¸ Document very large ({token_estimate:,} tokens)")
            logger.warning("âš ï¸ May exceed GPT-4o context window (128K tokens)")
        
        # âœ… ONLY 2 LLM CALLS:
        # 1. Generate long summary directly from text using your existing context and prompts
        long_summary = self._extract_full_context(
            text=text,
            raw_text=raw_text,
            doc_type=doc_type,
            fallback_date=fallback_date
        )

        # 2. Generate short summary from long summary
        short_summary = self._generate_short_summary_from_long_summary(long_summary, self.mode)

        logger.info("=" * 80)
        logger.info("âœ… QME EXTRACTION COMPLETE (2 LLM CALLS ONLY)")
        logger.info("=" * 80)

        # Return dictionary with both summaries
        return {
            "long_summary": long_summary,
            "short_summary": short_summary
        }

    
    def _extract_full_context(
        self,
        text: str,
        raw_text: str,
        doc_type: str,
        fallback_date: str
    ) -> str:
        """
        Generate long summary with PRIORITIZED context hierarchy:
        1. PRIMARY SOURCE: raw_text (accurate Document AI summarized context)
        2. SUPPLEMENTARY: text (full OCR extraction for missing details only)
        
        This ensures accurate context preservation while capturing all necessary details.
        """
        logger.info("ðŸ” Processing document with DUAL-CONTEXT approach...")
        logger.info(f"   ðŸ“Œ PRIMARY SOURCE (raw_text): {len(raw_text):,} chars (accurate context)")
        logger.info(f"   ðŸ“„ SUPPLEMENTARY (full text): {len(text):,} chars (detail reference)")
        
        # Build system prompt with CLEAR PRIORITY INSTRUCTIONS
        system_prompt = SystemMessagePromptTemplate.from_template("""
    You are an expert medical-legal documentation specialist analyzing a COMPLETE QME/AME/IME report.

    ðŸŽ¯ CRITICAL CONTEXT HIERARCHY (HIGHEST PRIORITY):

    You are provided with TWO versions of the document:

    1. **PRIMARY SOURCE - "ACCURATE CONTEXT" (raw_text)**:
    - This is the MOST ACCURATE, context-aware summary generated by Google's Document AI foundation model
    - It has been intelligently processed to preserve CRITICAL MEDICAL-LEGAL CONTEXT
    - **USE THIS AS YOUR PRIMARY SOURCE OF TRUTH**
    - This contains the CORRECT clinical interpretations, accurate medical findings, and proper context
    - **ALWAYS PRIORITIZE information from this source**

    2. **SUPPLEMENTARY SOURCE - "FULL TEXT EXTRACTION" (text)**:
    - This is the complete OCR text extraction (may have formatting noise, OCR artifacts)
    - Use ONLY to fill in SPECIFIC DETAILS that may be missing from the accurate context
    - Examples of acceptable supplementary use:
        * Exact medication dosages if not in primary source
        * Specific claim numbers or identifiers
        * Additional doctor names mentioned
        * Precise dates or measurements
    - **DO NOT let this override the clinical context from the primary source**

    âš ï¸ ANTI-HALLUCINATION RULES FOR DUAL-CONTEXT:

    1. **CONTEXT PRIORITY ENFORCEMENT**:
    - When both sources provide information about the SAME clinical finding:
        âœ… ALWAYS use interpretation from PRIMARY SOURCE (accurate context)
        âŒ NEVER override with potentially inaccurate full text version
    
    Example:
    - PRIMARY SOURCE says: "Patient has reached MMI with moderate functional limitations"
    - FULL TEXT says: "Patient improving with ongoing treatment" (may be from different section/context)
    âœ… CORRECT: Use "Patient has reached MMI with moderate functional limitations" (from primary)
    âŒ WRONG: Use "Patient improving" (ignores primary source context)

    2. **MEDICATION EXTRACTION WITH PRIORITY**:
    - FIRST: Extract medications from PRIMARY SOURCE (accurate context)
    - THEN: Check FULL TEXT only for:
        * Exact dosages if not specified in primary
        * Additional medications clearly marked as "current"
    - DO NOT extract medications from full text if they contradict primary source

    3. **DIAGNOSIS PRIORITY**:
    - PRIMARY SOURCE provides accurate diagnostic context
    - Use FULL TEXT only to add ICD-10 codes or body part specifics if missing
    - NEVER change diagnosis interpretation based on full text alone

    4. **PHYSICAL EXAM FINDINGS**:
    - PRIMARY SOURCE contains accurate, contextually relevant findings
    - Use FULL TEXT only for specific measurements (ROM degrees, strength scores) if missing
    - DO NOT add normal findings from full text if primary source focuses on abnormalities

    5. **RECOMMENDATIONS & MMI/WPI**:
    - These are CRITICAL medical-legal conclusions
    - **ALWAYS prioritize PRIMARY SOURCE** for these fields
    - Use FULL TEXT only if primary source has no information at all
    - NEVER mix interpretations from both sources

    6. **CLAIM NUMBERS & IDENTIFIERS**:
    - These are often in headers/footers (better in FULL TEXT)
    - Check FULL TEXT first for exact claim numbers
    - Use PRIMARY SOURCE if full text is unclear

    ðŸ” EXTRACTION WORKFLOW:

    Step 1: Read PRIMARY SOURCE (accurate context) thoroughly
    Step 2: Extract ALL clinical information, diagnoses, recommendations, MMI/WPI from PRIMARY SOURCE
    Step 3: Check SUPPLEMENTARY SOURCE (full text) ONLY for:
    - Specific details missing from primary (exact dosages, measurements)
    - Administrative info (claim numbers, exact dates)
    - Additional doctor names
    Step 4: Verify no contradictions between sources (if conflict, PRIMARY wins)

    âš ï¸ CRITICAL ANTI-HALLUCINATION RULES (HIGHEST PRIORITY) (donot include in output, for LLM use only):

    1. **EXTRACT ONLY EXPLICITLY STATED INFORMATION**
    - If a field/value is NOT explicitly mentioned in PRIMARY SOURCE, check SUPPLEMENTARY
    - If still not found, return EMPTY string "" or empty list []
    - DO NOT infer, assume, or extrapolate information
    - DO NOT fill in "typical" or "common" values

    2. **MEDICATIONS - ZERO TOLERANCE FOR ASSUMPTIONS**
    - Extract ONLY medications explicitly listed as "current" in PRIMARY SOURCE
    - Supplement with exact dosages from FULL TEXT only if dosage missing
    - DO NOT extract discontinued medications from either source

    3. **EMPTY FIELDS ARE ACCEPTABLE**
    - Better to return empty field than guess
    - DO NOT use phrases like "Not mentioned", "Not stated" - just return ""

    4. **EXACT QUOTES FOR CRITICAL FIELDS**
    - For MMI status, WPI, Work Restrictions: use EXACT wording from PRIMARY SOURCE
    - DO NOT paraphrase or interpret
    - If not in primary, check full text, but maintain original wording

    5. **QME PHYSICIAN/AUTHOR DETECTION**:
    - Check PRIMARY SOURCE first for author/signing physician
    - If not clear, scan FULL TEXT signature blocks (usually last pages)
    - Extract name as explicitly signed, regardless of credentials

    6. **ALL DOCTORS EXTRACTION**:
    - Extract from BOTH sources (primary + supplementary)
    - Deduplicate: If same doctor in both, use primary source version
    - Include all physicians with credentials

    7. **CLAIM NUMBER EXTRACTION**:
    - Check FULL TEXT headers/footers first (claim numbers often here)
    - Scan for patterns: "[Claim #XXXXXXXXX]", "Claim Number:", "WC Claim:"
    - Verify claim number is actual claim (not chart number or other ID)
    - if in the structured raw_text like json formatted dat, if the fileds are first and values then handle the same way to extract the claim number of accurate filed, but most of the time the fields are first and values are second then the claim number will be in the second field


    **NOW GENERATE A COMPREHENSIVE LONG SUMMARY IN MARKDOWN FORMAT**

    Follow the exact structure from your existing prompt, using the DUAL-CONTEXT PRIORITY approach described above.

    ## PATIENT INFORMATION
    - **Name:** [from primary source, supplement if needed]
    - **Date of Birth:** [from primary source, supplement if needed]
    - **Claim Number:** [from full text headers/footers first]
    - **Date of Injury:** [from primary source]
    - **Employer:** [from primary source, supplement if needed]

    ## REPORT DETAILS
    - **Report Type:** [QME/AME/IME]
    - **Report Date:** [from primary source]
    - **Evaluating Physician:** [from primary source, check full text signature if needed]

    Author:
    â€¢ Signature: [check primary first, then full text signature blocks if unclear]

    ## All Doctors Involved:
    â€¢ [extract from BOTH sources, deduplicate, prefer primary source format]

    ## DIAGNOSIS
    - [PRIMARY SOURCE for diagnosis interpretation and context]
    - [FULL TEXT only for ICD-10 codes if missing]

    ## PHYSICAL EXAMINATION FINDINGS
    - [PRIMARY SOURCE for clinically significant findings]
    - [FULL TEXT only for specific measurements if missing from primary]

    ## CLINICAL STATUS
    - [PRIMARY SOURCE for clinical interpretation]
    - [FULL TEXT only for specific dates, pain scores if missing]

    ## MEDICATIONS
    - **Current Medications:** [PRIMARY SOURCE first, supplement exact dosages from full text if needed]
    - **Previous Medications:** [PRIMARY SOURCE interpretation]
    - **Future Medications:** [PRIMARY SOURCE recommendations]

    ## MEDICAL-LEGAL CONCLUSIONS
    - [âš ï¸ HIGHEST PRIORITY: USE PRIMARY SOURCE ONLY for MMI, WPI, Work Restrictions]
    - [FULL TEXT only if primary source has zero information]

    ## RECOMMENDATIONS
    - [PRIMARY SOURCE for all recommendations]
    - [FULL TEXT only for additional specific details like test names, frequencies]

    Use markdown formatting with clear headings and bullet points. Be comprehensive but concise.
    """)

        # Updated user prompt to clearly separate both sources
        user_prompt = HumanMessagePromptTemplate.from_template("""
    â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
    ðŸ“Œ PRIMARY SOURCE - ACCURATE CONTEXT (Use this as your MAIN reference):
    â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

    {document_actual_context}

    â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
    ðŸ“„ SUPPLEMENTARY SOURCE - FULL TEXT EXTRACTION (Use ONLY for missing details):
    â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

    {full_document_text}

    â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

    Document Type: {doc_type}
    Report Date: {fallback_date}

    Generate the comprehensive long summary now following the DUAL-CONTEXT PRIORITY rules above.

    REMEMBER: 
    1. PRIMARY SOURCE (accurate context) is your MAIN reference
    2. Use FULL TEXT only to supplement specific missing details
    3. NEVER override primary source clinical interpretations with full text
    """)

        chat_prompt = ChatPromptTemplate.from_messages([system_prompt, user_prompt])
        
        try:
            start_time = time.time()
            logger.info("ðŸ¤– Invoking LLM with DUAL-CONTEXT PRIORITY approach...")
            logger.info("   ðŸ“Œ Primary: Accurate context from Document AI Summarizer")
            logger.info("   ðŸ“„ Supplementary: Full OCR text for detail enrichment")
            
            # Single LLM call with both sources
            chain = chat_prompt | self.llm
            response = chain.invoke({
                "document_actual_context": raw_text,  # PRIMARY: Accurate summarized context
                "full_document_text": text,           # SUPPLEMENTARY: Full OCR extraction
                "doc_type": doc_type,
                "fallback_date": fallback_date
            })
            
            long_summary = response.content.strip()
            
            end_time = time.time()
            processing_time = end_time - start_time
            
            logger.info(f"âš¡ Long summary generated in {processing_time:.2f}s")
            logger.info(f"âœ… Generated long summary: {len(long_summary):,} chars")
            logger.info("âœ… Context priority maintained: PRIMARY source used for clinical findings")
            
            return long_summary
            
        except Exception as e:
            logger.error(f"âŒ Long summary generation failed: {e}", exc_info=True)
            
            # Check if context length exceeded
            if "context_length_exceeded" in str(e).lower() or "maximum context" in str(e).lower():
                logger.error("âŒ Document exceeds GPT-4o 128K context window")
                return self._get_fallback_summary(doc_type, fallback_date)
            
            raise


    def _generate_short_summary_from_long_summary(self, long_summary: str, mode: str) -> str:
        """
        Generate a precise 30â€“60 word, pipe-delimited actionable summary in key-value format using LLM, tailored to mode.
        """
        logger.info(f"ðŸŽ¯ Generating LLM-based short summary tailored to mode: {mode.upper()} (30-60 words)...")

        mode_focus = ""
        if mode.lower() == "gm":
            mode_focus = """
    MODE: GENERAL MEDICINE (GM)
    - Focus on: Diagnosis, Pain/Clinical Status, Medications, Treatment Recommendations
    - Keys: Diagnosis | Clinical Status | Medications | Recommendations
    - Clinical emphasis, actionable for care planning
    """
        elif mode.lower() == "wc":
            mode_focus = """
    MODE: WORKERS COMPENSATION (WC)
    - Focus on: MMI/WPI, Work Restrictions, Impairment, Claim-Relevant Recommendations
    - Keys: MMI Status | WPI | Work Status | Legal Recommendations
    - Legal/claims emphasis, concise for adjusters
    """
        else:
            mode_focus = "MODE: DEFAULT - Balanced clinical and legal keys"

        system_prompt = SystemMessagePromptTemplate.from_template(f"""
    You are a medical-legal extraction specialist generating mode-tailored short summaries.

    {mode_focus}

    STRICT REQUIREMENTS:
    1. Word count MUST be between **30 and 60 words** (min 30, max 60).
    2. Format MUST be EXACTLY a single pipe-delimited line:
    - **ONLY include, critical, or clinically significant findings**.
    - **ONLY include abnormalities or pathological findings for physical exam and vital signs (if present). Skip normal findings entirely for these (physical exam, vital signs) fields.**
    - For Author never use "Dr." with it

    
    Make sure to follow this EXACT format:
    [Report Title] | [Author] | Date:[value] | Body Parts:[value] | Diagnosis:[value] | Physical Examination:[value (only critical findings or abnormalities else skip)] | Vital Signs:[value (only critical vital signs else skip)] | Treatment Plan:[value] | Critical Finding:[value (only abnormalities) if applicable] | Follow-up:[value] | Recommendations:[value] | Work Status:[value] | MMI Status:[value] | WPI:[value] | Medications:[value]

    3. DO NOT fabricate or infer missing data â€” simply SKIP entire key-value pairs that do not exist.
    4. Use ONLY information explicitly found in the long summary.
    5. Output must be a SINGLE LINE (no line breaks).
    6. Prioritize mode-specific keys first, then general (Body Parts, Diagnosis).
    7. ABSOLUTELY NO: assumptions, invented data, narrative sentences.
    8. If a field is missing, SKIP THE ENTIRE KEY-VALUE PAIRâ€”do NOT include empty pairs OR placeholders like:
    - "not included"
    - "not listed"
    - "no abnormalities detected"
    - "not provided"
    - "not discussed"

    ABSOLUTELY FORBIDDEN:
- Normal findings (ignore them entirely)
- For Author never use "Dr." with it
- assumptions, interpretations, invented medications, or inferred diagnoses
- placeholder text or "Not provided"
- narrative writing
- duplicate pipes or empty pipe fields (e.g., "||")
- any patient details (patient name, DOB, ID)

    Your final output must be 30â€“60 words and MUST follow the exact format.
        """)

        user_prompt = HumanMessagePromptTemplate.from_template("""
    LONG SUMMARY:

    {long_summary}

    Now produce the 30â€“60 word single-line summary following the strict mode-tailored rules.
        """)

        chat_prompt = ChatPromptTemplate.from_messages([system_prompt, user_prompt])

        try:
            chain = chat_prompt | self.llm
            response = chain.invoke({"long_summary": long_summary})
            summary = response.content.strip()

            # Clean whitespace only
            summary = re.sub(r'\s+', ' ', summary).strip()

            # ðŸš€ Extra Cleanup: Remove any "not included / not provided / no X" pairs
            forbidden_phrases = [
                "not included", "not listed", "not provided", "not discussed",
                "no abnormalities detected", "no critical findings", "none", "N/A"
            ]

            segments = [seg.strip() for seg in summary.split("|")]
            cleaned_segments = []

            for seg in segments:
                if any(bad in seg.lower() for bad in forbidden_phrases):
                    continue  # skip this key-value entirely
                cleaned_segments.append(seg)

            summary = " | ".join(cleaned_segments).strip()
            
            # Programmatically add missing Date or Author if LLM missed them
            summary = ensure_date_and_author(summary, long_summary)

            # Word count check
            wc = len(summary.split())
            if wc < 30 or wc > 60:
                logger.warning(f"âš ï¸ Summary outside word limit ({wc} words). Attempting auto-fix.")
                fix_prompt = ChatPromptTemplate.from_messages([
                    SystemMessagePromptTemplate.from_template(
                        f"Your previous summary had {wc} words. Rewrite it to be STRICTLY between 30 and 60 words while preserving accuracy, mode focus, exact format, and skipping missing keys."
                    ),
                    HumanMessagePromptTemplate.from_template(summary)
                ])
                chain2 = fix_prompt | self.llm
                fixed = chain2.invoke({})
                summary = re.sub(r'\s+', ' ', fixed.content.strip())

                # Remove placeholders after fix as well
                segments = [seg.strip() for seg in summary.split("|")]
                cleaned_segments = []
                for seg in segments:
                    if any(bad in seg.lower() for bad in forbidden_phrases):
                        continue
                    cleaned_segments.append(seg)
                summary = " | ".join(cleaned_segments).strip()
                # Re-ensure date and author after correction
                summary = ensure_date_and_author(summary, long_summary)

                logger.info(f"ðŸ”§ Fixed summary word count: {len(summary.split())} words")

            logger.info(f"âœ… Final short summary: {len(summary.split())} words")
            return summary

        except Exception as e:
            logger.error(f"âŒ Short summary generation failed: {e}")
            return "Summary unavailable due to processing error."

    def _get_fallback_summary(self, doc_type: str, fallback_date: str) -> str:
        """Fallback summary when processing fails"""
        return f"""## {doc_type} Report
**Report Date:** {fallback_date}

**Note:** Comprehensive analysis unavailable due to processing limitations. 
Please review the original document for complete details."""